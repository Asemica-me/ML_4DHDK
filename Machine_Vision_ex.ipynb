{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.5.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import os, random\n",
    "import pandas as pd\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42) # Setting the seed\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 \n",
    "\n",
    "Modify the code to use different pre-trained architectures. [See here for ideas.](https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "https://pytorch.org/vision/stable/models/generated/torchvision.models.inception_v3.html#torchvision.models.Inception_V3_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - images shape: torch.Size([32, 3, 299, 299]), labels shape: torch.Size([32])\n",
      "Validation - images shape: torch.Size([32, 3, 299, 299]), labels shape: torch.Size([32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 157\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    160\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minceptionv3_model.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 116\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Calculate loss using primary outputs only\u001b[39;00m\n\u001b[0;32m    115\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m    119\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Lucrezia\\OneDrive - Alma Mater Studiorum Università di Bologna\\Machine Learning\\Repository\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucrezia\\OneDrive - Alma Mater Studiorum Università di Bologna\\Machine Learning\\Repository\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucrezia\\OneDrive - Alma Mater Studiorum Università di Bologna\\Machine Learning\\Repository\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create label-to-index mapping\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(self.annotations['label'].unique())}\n",
    "        self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n",
    "\n",
    "        # Convert labels to integers\n",
    "        self.annotations['label'] = self.annotations['label'].map(self.label_to_index)\n",
    "\n",
    "        # Compute class weights based on label frequency\n",
    "        class_counts = self.annotations['label'].value_counts().sort_index()\n",
    "        total_samples = len(self.annotations)\n",
    "\n",
    "        # Calculate weights inversely proportional to class frequencies\n",
    "        self.class_weights = torch.tensor([total_samples / (len(class_counts) * count) for count in class_counts], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.annotations.iloc[index, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Paths to the data\n",
    "csv_file = './data/newspaper_images/ads_data/ads_upsampled_no_index.csv'  # Path to your CSV file\n",
    "img_dir = './data/newspaper_images/ads_data/images'       # Directory with all the images\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(20),  # Add padding to ensure images are large enough for the kernel\n",
    "    transforms.Resize((299, 299)),  # Resize to 299x299 to match Inception v3's input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization as usual\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomImageDataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\n",
    "\n",
    "# Define the train-validation split\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "val_size = len(dataset) - train_size  # Remaining 20% for validation \n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for train and validation datasets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the training data\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Train - images shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "    break\n",
    "\n",
    "# Example: Iterate through the validation data\n",
    "for images, labels in val_loader:\n",
    "    print(f\"Validation - images shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "    break\n",
    "\n",
    "# ================= MODEL, TRAINING, EVALUATION ===================== #\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "model = models.Inception3(init_weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True) # same model architecture and pre-trained weights\n",
    "\n",
    "# Freeze the layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"Mixed_7\" in name:  # Unfreeze layers from Mixed_7 block\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Replace the last layer to a new one, and match the number of classes\n",
    "num_features = model.fc.in_features  # Get input features of the final layer\n",
    "model.fc = nn.Linear(num_features, 2) \n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.0, 1.0], dtype=torch.float)\n",
    "criterion = nn.CrossEntropyLoss(weight=dataset.class_weights.to(device))\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001) # instead of SGD\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    model.to(device)  # Move model to the desired device\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # Forward pass\n",
    "\n",
    "            # Extract the primary output from the Inception model\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # Get the main output tensor if outputs is a tuple\n",
    "\n",
    "            # Calculate loss using primary outputs only\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update parameters\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs, _ = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        # Adjust learning rate based on scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device=device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'inceptionv3_model.tar')\n",
    "\n",
    "# Validate the model\n",
    "# Get a single batch from the validation loader\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "data_iter = iter(val_loader)\n",
    "images, true_labels = next(data_iter)\n",
    "\n",
    "# Move data to the device\n",
    "images = images.to(device)\n",
    "true_labels = true_labels.to(device)\n",
    "\n",
    "# Perform a forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    outputs, _ = model(images)\n",
    "    probabilities = F.softmax(outputs, dim=1) # calculating probabilities from the logits using the softmax function\n",
    "\n",
    "# Get the predicted label and confidence\n",
    "_, predicted_labels = torch.max(outputs, 1)\n",
    "confidence, _ = torch.max(probabilities, 1)\n",
    "\n",
    "# Randomly select an index for the image to display\n",
    "index = random.randint(0, images.size(0) - 1)\n",
    "image = images[index].cpu().permute(1, 2, 0).numpy()  # Convert to HxWxC format for plotting\n",
    "true_label = true_labels[index].item()\n",
    "predicted_label = predicted_labels[index].item()\n",
    "confidence_score = confidence[index].item()\n",
    "\n",
    "# Convert the label indexes back to string labels using the dataset's index_to_label dictionary\n",
    "true_label_str = val_dataset.dataset.index_to_label[true_label]\n",
    "predicted_label_str = val_dataset.dataset.index_to_label[predicted_label]\n",
    "\n",
    "# Print the true label, predicted label, and confidence\n",
    "print(f\"True Label: {true_label_str}\")\n",
    "print(f\"Predicted Label: {predicted_label_str}\")\n",
    "print(f\"Model Confidence: {confidence_score:.4f}\")\n",
    "\n",
    "# Optionally, display the image\n",
    "plt.imshow(image)\n",
    "plt.title(f\"True: {true_label_str}, Predicted: {predicted_label_str}, Confidence: {confidence_score:.4f}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - WikiArt Dataset\n",
    "\n",
    "Link to Dataset [here](https://github.com/cs-chan/ArtGAN/blob/master/WikiArt%20Dataset/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model implemented is for classifying images from the WikiArt dataset. It uses a transfer learning approach with MobileNetV2, a lightweight convolutional neural network (CNN) architecture that is particularly suited for mobile and embedded vision applications. This model is efficient in terms of both memory and computation, making it a good choice for environments with limited resources.\n",
    "\n",
    "Inverted Residuals: It introduces a concept called inverted residuals with linear bottlenecks. This means that the network first expands the feature dimensions using lightweight depthwise separable convolutions and then reduces them back, preserving important information while keeping computational costs low.\n",
    "\n",
    "Depthwise Separable Convolutions: These convolutions reduce the number of parameters and computational cost by separating the spatial and channel dimensions, which leads to a more efficient network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "\n",
    "Input Layer: The model expects input images resized to the standard size of 224×224 pixels. \n",
    "\n",
    "Base Layers: The bulk of the MobileNetV2 architecture consists of several layers of depthwise separable convolutions, which gradually extract features from the input images. The architecture is deep enough to learn complex representations while remaining lightweight.\n",
    "\n",
    "Output Layer: The final classification layer is modified to match the number of output classes (genres in your case). For instance, if there are 6 genres, the final layer would output 6 logits (raw prediction scores for each class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logic\n",
    "\n",
    "Data Preparation: The images are loaded and augmented (e.g., resized, flipped) to improve generalization. Augmentations are particularly useful to prevent overfitting by artificially enlarging the training dataset.\n",
    "\n",
    "Loss Function: The model uses Cross Entropy Loss, which is commonly used for multi-class classification problems. It measures the difference between the predicted probability distribution and the true distribution.\n",
    "\n",
    "Optimizer: Adam is used as the optimizer. It's an adaptive learning rate optimization algorithm that is computationally efficient and well-suited for problems with a large number of parameters.\n",
    "\n",
    "Training Loop:\n",
    "The model is set to training mode and iterates through batches of training data.\n",
    "For each batch, it performs a forward pass, computes the loss, and updates the model weights through backpropagation.\n",
    "After training on the entire dataset, the model's performance is validated on the validation set to check for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluation and Saving the Model\n",
    "\n",
    "After training, the model is evaluated based on its accuracy and loss on the validation set.\n",
    "Finally, the model's learned weights are saved to a file (mobilenetv2_wikiart.pth), allowing to reload it and use the model for inference or further training in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (genres): ['Abstract_Expressionism', 'Action_painting', 'Analytical_Cubism', 'Art_Nouveau_Modern', 'Baroque', 'Color_Field_Painting', 'Contemporary_Realism', 'Cubism', 'Early_Renaissance', 'Expressionism', 'Fauvism', 'High_Renaissance', 'Impressionism', 'Mannerism_Late_Renaissance', 'Minimalism', 'Naive_Art_Primitivism', 'New_Realism', 'Northern_Renaissance', 'Pointillism', 'Pop_Art', 'Post_Impressionism', 'Realism', 'Rococo', 'Romanticism', 'Symbolism', 'Synthetic_Cubism', 'Ukiyo_e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucrezia\\OneDrive - Alma Mater Studiorum Università di Bologna\\Machine Learning\\Repository\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lucrezia\\OneDrive - Alma Mater Studiorum Università di Bologna\\Machine Learning\\Repository\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\Lucrezia/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:04<00:00, 3.17MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "image_dir = r\"C:\\Users\\Lucrezia\\OneDrive - Alma Mater Studiorum Università di Bologna\\Machine Learning\\Repository\\data\\WikiData\\wikiart_imgs\"\n",
    "\n",
    "# Get all subfolders (genres/classes)\n",
    "classes = os.listdir(image_dir)\n",
    "print(f\"Classes (genres): {classes}\")\n",
    "\n",
    "# Collect image paths and their labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for idx, genre in enumerate(classes):\n",
    "    genre_folder = os.path.join(image_dir, genre)\n",
    "    for img_file in os.listdir(genre_folder):\n",
    "        if img_file.endswith(('.jpg', '.jpeg', '.png')):  # Adjust for valid image extensions\n",
    "            image_paths.append(os.path.join(genre_folder, img_file))\n",
    "            labels.append(idx)  # Label is the index of the genre\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% val)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset to load images from file paths and labels\n",
    "class WikiArtDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transformations for training and validation datasets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = WikiArtDataset(img_paths=train_paths, labels=train_labels, transform=data_transforms['train'])\n",
    "val_dataset = WikiArtDataset(img_paths=val_paths, labels=val_labels, transform=data_transforms['val'])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Modify the final layer to match the number of classes (in this case, the number of genres)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, len(classes))  # len(classes) is the number of genres\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            loader = train_loader if phase == 'train' else val_loader\n",
    "            dataset_size = len(train_dataset) if phase == 'train' else len(val_dataset)\n",
    "\n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Track loss and accuracy\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            epoch_acc = running_corrects.double() / dataset_size\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model_trained = train_model(model, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(model_trained.state_dict(), 'mobilenetv2_wikiart.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
